{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "* what are activation functions ?\n",
    "* Types of activation functions in neural network\n",
    "* Most widely used activation function\n",
    "* How to choose and activation function for our analysis?\n",
    "* Coding activation function in python\n",
    "\n",
    "1. What are activation functions ?\n",
    "   Activation functions are a critical part of the design of a neural network\n",
    "\n",
    "   An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network\n",
    "\n",
    "   Sometimes the activation function is called a `transfer function`\n",
    "\n",
    "   If the output range of the activation function is limited then it may be called a sequashing function\n",
    "\n",
    "   Many activation function are nonlinear and may be reffered to as the nonlinearity in the layer or the network design.\n",
    "\n",
    "2. why Activation function?\n",
    "\n",
    " Tone of data is available  \n",
    "   * Data is non-structured\n",
    "   * No define line between useful and non-useful data\n",
    "   * Noisy data will not give required output\n",
    "\n",
    "**Activation function help us to deal with it**\n",
    "*Activation function are mathematical equations thath determine the output of a nueral network model. They help the network to use the importat information and suppress the noise*\n",
    "\n",
    "* In a neural netwok, activation function are utilized to bring no-linearities into the decision border.\n",
    "* The goal of introducing nonlinearities in data is to simulate a real-world situation.\n",
    "  \n",
    "  **Almost much of the data we deal with in real life is nonlinear. This is what makes neural networks so effective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Activation Function\n",
    "\n",
    "![](linearactivationjpg.jpg)\n",
    "\n",
    "The linear activation function, also known as `no activation` or `identity function` is where the activation is proportional to the input \n",
    "\n",
    "The function does not anything to the weighted sum of the input, it simply spits out the value it was given as shown prevously\n",
    "\n",
    "f(x) = x  # it is also use as for regression problem \n",
    "\n",
    "**A linear activation function has two major problems**\n",
    "\n",
    "* It is not possible to use backpropagation as the derivative of the function is a constant and has no relation to input X.\n",
    "* All layers of the neural network will collapse into one if a linear activation function is used.No matter the number of layers in the nueral netwok, the last layer will still be a linear function of the first layer.\n",
    "* So essentially a linear activation function turns the neural network into just one layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Activation functions\n",
    "\n",
    "The limited power linear activation function (simply a linear regression model), this does not allow the model to create complex mappings between the networks inputs and outputs.\n",
    "\n",
    "**Non-linear activation function solve the sfollowing limitation of linear activation function**\n",
    "1. They allow backpropogation because now the derivative function would be related to the input and it possibel to go back and understand which weight in the input neurons can provide a better prediction.\n",
    "2. They allow the stacking of multiple layer of neurons as teh output would now be a non-linear combination of input passed through multiple layers. any output can be reresented as a functional computation in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary step Function\n",
    "\n",
    "![](binnaryactivation.jpg)\n",
    "**Binary step function is threshold-based activation function which means**\n",
    "\n",
    "* after a certain threshold neurons is activated\n",
    "* below the said threshold neurons is deactivated\n",
    "\n",
    "*This actiavation function can be used in binary classsification as the name suggest, however it can not be used in a situation where you have multiple classes to deal with*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid/Logistic Function\n",
    "\n",
    "![](sigmoidpng.png)\n",
    "* Smooth curve between 0 and 1 \n",
    "* This function takes any real value a input and output values in the range of 0 to 1\n",
    "* The larger the input the closer the outpput value will be the 1 whereas the smaller the input more negative the closes the output eill be the 0 as shown below.\n",
    "\n",
    "f(x) = 1/1+e^-x\n",
    "\n",
    "### Advantages:\n",
    "sigmoid function is most widely used :\n",
    "* It is commonoly used for models where we have to predict the probablity as an output since probablity of anything exists only between the range 0 and 1 sigmoid is the right choice becaus eof its range.\n",
    "* It normalize the output of each neuron \n",
    "* The function is different and provides a smooth gradient prevnting jumps in output values this is represented by an S-shape of the sigmoid aactivation functions\n",
    "\n",
    "### Disadvantages\n",
    "* However, Sigmoid function makes almost no change in the prediction for very high or very low inputs which ultimately results in nueral network refusing to learn further This problem is Known as the Vanishing graidient.\n",
    "* Computational expensive\n",
    "* Outputs not zero centered \n",
    "* vanshing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Function (Hyperbolic Tangent)\n",
    "![](tanh.png)\n",
    "\n",
    "* Very similar to the sigmoid/logistic activation functioon\n",
    "* even has a s shape\n",
    "* but the difference in the output range of -1 to 1 \n",
    "* In Tanh teh larger the input the closer the output value will be to 1 whereas the smaller the input more negative the closer the output will be to -1\n",
    "\n",
    "Tanh = f(x) = (e^x - e^-x)/(e^x + e^-x)\n",
    "\n",
    "### Advantages:\n",
    "1. Zero centered\n",
    "2. The prediction is simple based on probablity value\n",
    "   \n",
    "   *However, tanh also comes with the vanishing gradient problem just like sigmoid function*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu (REctified Linear Unit) Function\n",
    "![](ReLU-activation-function.png)\n",
    "\n",
    "* In this function, outputs fot he psitive input can range from 0 to infinity,\n",
    "* Although it give an impression of a linear function,ReLU has a derivative function and alows for backpropagation while simultaneously making it computationally efficient.\n",
    "* The main catch here is that the ReLU function does not activate all neurons at the same time \n",
    "* The neurons will only be deactivated if the output of the linear transformation is lesss than 0\n",
    "  \n",
    "Relu = f(x) = max(0,x)\n",
    "\n",
    "### Advantages\n",
    "* Since only a certain number of neurons are acivated, Relu function is far more computationally efficient when compared to sigmoid and tanh function.\n",
    "* ReLU accelerate teh convergence of gradient descent toward the golbal minimum of the loss function due to its linear, non-saturating property\n",
    "  \n",
    "### Disadvantage\n",
    "* When the input is zero or a negative value the function output zero and it hinders with the back-propogation. This problem is known as the dying ReLU problem\n",
    "  \n",
    "# Leaky ReLU (pro of ReLU Function and solve the dying issue of ReLU function)\n",
    "![](leakyrelu.jpg)\n",
    "* Leaky ReLU is the most common and effective method to solve a dying ReLU problem\n",
    "* It is nothing but an imporoved version of the ReLU function\n",
    "* It add a slight slope in the negative range to prevent the dying ReLU issu\n",
    "  \n",
    "  Leaky ReLU = f(x) = max(0.1x,x)\n",
    "\n",
    "### Advantages:\n",
    "* Same as the that of ReLU\n",
    "* In Addition to the fact taht it does enable backporpogation even for the negative input values.\n",
    "* By making this minor modification for negative input values, the gradient of the left side of the graph comes out to be a non-zero value. Therefore, we would no longer encounter dead neurons in that region\n",
    "  ### Disadvantage\n",
    "* When the input is zero or a negative value, the functionoutput zero and it hinder with the back-propogation.This problem is known as the dying ReLU problem.\n",
    "\n",
    "# Parametric ReLU\n",
    "![](parametricrelu.webp)\n",
    "## Advantages:\n",
    "* Parametric ReLU is another variant of ReLU that aims to solve the problem of graident becoming zero for the left half of the axis.\n",
    "* This function provides the slope of the negative part of the function as an arguments a By performing backpropogation, the most appropriate value of a is learnt.\n",
    "* The parametric ReLU function is used when the leaky ReLU function still fails at solving the problem of dead neurons, and the relevant information is not successfully passed to the next layer.\n",
    "  \n",
    "## Disadvantages:\n",
    "* This function limitaion is that it may perform differently for different problem depending upon the value of slope parameter a.\n",
    "  \n",
    " parametric ReLU= f(x) = max(ax,x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax activation function\n",
    "* The softmax activation function is the genralized form of the sigmoid function for the multiple dimentions.\n",
    "* It is the mathematical function that convert the vector of numbers into the vector of the probablities.\n",
    "* The softmax activation function is commonoly used as an activation functionin the case of multi-class classification problem in the machine learning \n",
    "* The output of the softmax is interpreted as probablity of getting each class.\n",
    "* softmax function is described as a combination of multiple sigmoid\n",
    "* it calculate the relative probablities. similar to the sigmoid logistic activation function the softmax function return the probablities of each class .\n",
    "* it is comonolly used as an activation function for the last layer of the neural network in the case of multi-class classification.\n",
    "  \n",
    "  ## Advantages:\n",
    "  * It can be used for multiclass classification\n",
    "  * IT normalize the output for each class between 0 and 1 and divides by their sum, giving the probablities of the input value being in a specific class\n",
    "  * For neural network that need to categorize input into numerous categories , softmax often employed exclusively for the output layer of the neural network\n",
    "\n",
    "  Softmax (z) = exp(Z)/sum exp(z)\n",
    "  \n",
    "*The output of the sigmoid function was in the range of 0  to 1 which can be thought of as probablity but*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose activation functions:\n",
    "**Genral Rules to use activation functions**\n",
    "use sigmoid function in output layer, use tanh in all other places. Both of these have vanishing the gradient problem\n",
    "\n",
    "So Now\n",
    "* Therefore, Relu can be use in case of both above \n",
    "* ReLU is most popular used function for hidden layer.\n",
    "* computationally effective math due to simplicity.\n",
    "*  However, it also has vanishing gradient problem.\n",
    "  **Leaky ReLU or Parametric ReLU can be used here then to reduce the computation**\n",
    "\n",
    "## Activation function for Hidden layers:\n",
    "A `hidden layer` in a neural network is a layer that recieve input from another layer such as onother layer or an input layer and provides output to another layer such as another layer or an output layer \n",
    "\n",
    "* A hidden layer does not directly contact input data or prudece output for a model, at least in general\n",
    "* A neural network may have zero or more hidden layers \n",
    "* Typically , a differentiabe non-linear activation function is used in the hidden layer of aneural netwrk. This allows the model to learn more complex function than a network trained using a linear activation function\n",
    "\n",
    "**So most widely used activation function for hidden layer**\n",
    "* Rectified Linear Activation (ReLU)\n",
    "* Logistic(Sigmoid)\n",
    "* Hyperbolic Tangent(tanh)\n",
    "* ReLU is the most widely used activation function used for the hidden layer.\n",
    "* Simple to implement and effective at overcomming the limitation Sigmoid and tanh.\n",
    "* it suffer in vaishing gradients or prevent the deep models from being trained.\n",
    "* Althogh it can suffer from other problem like saturatd or dead unitss\n",
    "* so we used Leaky ReLU or Parametric ReLU\n",
    "\n",
    "**Note :**\n",
    " *A neural network will always have the same activation function in all hidden layers*\n",
    "\n",
    "* **Modern neural network models with common architectures, such as MLP and CNN will make use of the ReLU activation function or extention**\n",
    "* **Recurrent networks(RNN) still commonly use Tanh or sigmoid activation functions or even both**\n",
    "* **The LSTM(long short term memory) commonoly uses the Sigmoid activation for recurrent connection and the Tanh activation for output**\n",
    "  \n",
    "## activation function for output layers:\n",
    "* The output layer is the layer in neural network model taht directly outputs a prediction.\n",
    "* All feed-forward neural network models have an output layers.\n",
    "* Most widely used activation function for the output layers:\n",
    "        * Linear Activation function\n",
    "        * Logistic (sigmoid)\n",
    "        * softmax"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
